---
title: "UWDS2-Wk8-ImprovedLrmcModel-jms206"
author: "Jim Stearns, NetID=jms206"
date: "Due 10 Mar 2015"
output: 
    pdf_document:
        latex_engine: xelatex
geometry: margin=0.5in
fontsize: 12pt
mainfont: Verdana
monofont: Consolas
---

```{r setup, echo=FALSE}
setwd("~jimstearns/GoogleDrive/Learning/Courses/UWPCE-DataScience/UWDS-Repo-JimStearns206/Course2_Methods/Week08")
```
# Improving the LRMC model for NCAA Basketball Ranking

(Assignment language is in _italics_)

## Overview
    
_This week we will combine the new feature you evaluated previously with the LRMC (Logistic Regression/Markov Chain) model to see if the combined approach can improve the base model.  Please write a summary of your approach for combining the two models as well as a statement about comparing the combined model’s performance to the basic model on the 2012 and 2013 data._
    
My approach was to follow the course suggested by the supplied NCAA_2014.R script, namely:
* Use the LRMC model as the base model to predict the results of the 2014 NCAA tournament.
* Try the RPI model by itself.
* Try a range of blends of the two models.

I wasn't able to target the models to the data for years 2012 and 2013. 
Reason: the RPI values are on the Rankings page, and I couldn't find one for earlier years.
http://www.cbssports.com/collegebasketball/rankings/rpi/index1 gives the current year.
So I'm actually using 2015 rankings, aren't I?

__Are there URLs to the CBS Sports website with rankings for previous years?__

```{r read-in-but-dont-echo-Rsource, echo=FALSE}
# Suppress the output of parsing and executing the statement in this R file - very verbose!
outanderrmsgs=file("tmpoutanderr.txt", open="at")
sink(file=outanderrmsgs, type=c("output", "message"))
source("NCAA_2014.R", echo=FALSE, print.eval=FALSE, verbose=FALSE)
#source("glomp.R", echo=FALSE, print.eval=FALSE, verbose=FALSE)
sink()
```

## Results: LRMC Model by Itself

```{r}
print("LRMC")
print(lmrcPredictions$correct)
print(lmrcPredictions$total)
print(lmrcPredictions$correct/lmrcPredictions$total)
```

## Results: RPI Model by Itself

```{r}
print("RPI Prediction Results")
print(rpiPredictions$correct)
print(rpiPredictions$total)
print(rpiPredictions$correct / rpiPredictions$total)
```
## Combining Models

_You may combine the new information with the base model in any way you like, however, here are two approaches that you may find useful…_

1.    _Combine final scores_

_Suppose that the new factor you chose allows you to produce a ranking model independent of the basic LRMC model. For example suppose you chose RPI score, which immediately allows you to create an ordered list of all teams. Let x be a team’s LRMC score and y be its RPI score. The combined score could be a weighted average of these two, e.g.,  a*x + (1-a)*y where a is a number between 0 and 1. For numerical stability, it will also be helpful in this case to make sure that the two scores have similar magnitude (i.e., both have similar minimum and maximum values). If they do not, then rescale one of them as necessary (consider the R function “scale”)._

Scaling was used so that RPI and LRMC have similar ranges (0..1).

2.	_Modify the probability matrix_

_The LRMC model works by first constructing a probability matrix (the value in row i and column j is the probability that team i is better than team j).  If you have a method for updating this probability based on your feature, then the matrix can be modified and then the regular LRMC ranking process can be applied. Again, a weighted average of the form a*x + (1-a)*y might be useful._

## Evaluation of New Model

_To judge the quality of each model we will use a relatively simple measurement: the number of head-to-head predictions it correctly makes for the games played during the 2014 NCAA tournament (you might also be interested in previous years, though this requires some tedious adjustments to the part of the LRMC code that collects the data). Only data available before the beginning of the tournament may be used to make predictions. If you have used an approach similar to the weighted average described above, you can now use this measurement to search for an optimal value of the weighting parameter._

```{r}
plot(seq(0,1,0.02), ratioCorrect, main="NCAA Predictions: RPI*alpha + LRMC*(1-alpha)", 
     xlab="alpha", ylab="Ratio of Correctly Predicted Results")

print(sprintf("Best prediction ratio of %0.3f achieved with alpha=%s (alpha*RPI + (1-alpha)*LRMC)", 
              bestAlphaRatio, bestAlpha))
```

## Appendix: R Source

The following source is a debugged version of the NOAA_2014.R script provided 
as part of this assignment with these changes:

* Activated URL checker, added a few special cases to the pairs in transformName1().
* Debugged and got it working.
    * Reports LRMC, RPI, and a blend of the two.
    * RPI doesn't help LRMC, so the blend as it stands is not a very interesting result.
* Stayed with LRMC base plus RPI, introducing scaling.
    Scaled both LRMC and RPI scores to range 0..1.
    At a weighting of 0.76 RPI (i.e. 0.24 weighting for LRMC),
    number of correctly predicted games improved from 47 (LRMC alone) to 49.
* Improved plot, added print of best alpha value (0.76 for year 2004).
* Refactor: Put prediction result calculators for RPI and LRMC into own functions.

```{r, echo=FALSE}
# Print the source of the R script without evaluation. Already evaluated above (with echo=FALSE).
readLines("NCAA_2014.R")
```